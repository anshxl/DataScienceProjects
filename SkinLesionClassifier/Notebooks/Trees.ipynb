{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "BASE_PATH = '/Users/AnshulSrivastava/Desktop/Fall24/CMSE 492/Project/isic-2024-challenge/'\n",
    "RANDOM_SEED = 69\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "metadata = pd.read_csv('metadata_with_cnn.csv')\n",
    "\n",
    "metadata = metadata.drop(columns='kfold', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the CNN confidence\n",
    "noise_level = 0.02\n",
    "\n",
    "metadata['cnn_confidence'] = metadata['cnn_confidence'] + np.random.normal(0, noise_level, metadata.shape[0])\n",
    "metadata['cnn_confidence'] = metadata['cnn_confidence'].clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add folds\n",
    "metadata['kfold'] = -1\n",
    "stratified_kfold = StratifiedGroupKFold(n_splits=N_FOLDS)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(stratified_kfold.split(X=metadata, y=metadata['target'], groups=metadata['patient_id'])):\n",
    "    metadata.loc[val_idx, 'kfold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encode 'age_group'\n",
    "metadata['age_group'] = metadata['age_group'].fillna('nan')\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "metadata['age_group'] = ordinal_encoder.fit_transform(metadata['age_group'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = metadata.columns.to_list()\n",
    "train_cols.remove('target')\n",
    "train_cols.remove('patient_id')\n",
    "train_cols.remove('kfold')\n",
    "train_cols.remove('isic_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lgbm_metric(y_true, y_pred):\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "    # Convert y_true to 0/1 format for binary classification\n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = np.array([1.0 - x for x in y_pred])\n",
    "\n",
    "    # Calculate partial AUC\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "\n",
    "    # Return the metric name, value, and maximization indicator\n",
    "    return 'pAUC', partial_auc, True\n",
    "\n",
    "def partial_auc_score(y_true, y_pred):\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "    # Convert y_true to 0/1 format for binary classification\n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = np.array([1.0 - x for x in y_pred])\n",
    "\n",
    "    # Calculate partial AUC\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "\n",
    "    # Return the metric name, value, and maximization indicator\n",
    "    return partial_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"custom\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"device\": \"cpu\",\n",
    "        'random state': RANDOM_SEED\n",
    "    }\n",
    "\n",
    "    # Initialize list to store AUC scores\n",
    "    aucs = []\n",
    "\n",
    "    # Iterate over folds\n",
    "    for fold in range(N_FOLDS):\n",
    "        # Get train and validation data\n",
    "        train_data = metadata[metadata['kfold'] != fold].reset_index(drop=True)\n",
    "        val_data = metadata[metadata['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Define features and target\n",
    "        features = train_data[train_cols]\n",
    "        target = train_data['target']\n",
    "\n",
    "        dtrain = lgb.Dataset(features, target)\n",
    "\n",
    "        # Train model\n",
    "        model = lgb.train(param, dtrain,)\n",
    "        preds = model.predict(val_data[train_cols])\n",
    "        auc = partial_auc_score(val_data['target'], preds)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-22 22:41:39,705] A new study created in memory with name: lgbm_optimization\n",
      "[I 2024-11-22 22:41:43,731] Trial 0 finished with value: 0.1601477163166451 and parameters: {'learning_rate': 0.02940714881841564, 'lambda_l1': 1.5856234362030003e-07, 'lambda_l2': 0.5499446320731312, 'num_leaves': 99, 'feature_fraction': 0.5873137275143606, 'bagging_fraction': 0.45172514662065333, 'bagging_freq': 2, 'min_child_samples': 14}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:41:47,772] Trial 1 finished with value: 0.1586062563423794 and parameters: {'learning_rate': 0.07855810851734218, 'lambda_l1': 0.045815518854756654, 'lambda_l2': 1.604684760338536e-08, 'num_leaves': 112, 'feature_fraction': 0.8202811046063471, 'bagging_fraction': 0.7202837813953901, 'bagging_freq': 5, 'min_child_samples': 96}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:41:51,003] Trial 2 finished with value: 0.15576599279103423 and parameters: {'learning_rate': 0.0003169504132495301, 'lambda_l1': 3.714566246720151e-07, 'lambda_l2': 0.0002709471441485622, 'num_leaves': 140, 'feature_fraction': 0.7195624180668064, 'bagging_fraction': 0.4351658760659959, 'bagging_freq': 2, 'min_child_samples': 45}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:41:53,018] Trial 3 finished with value: 0.15589763824111702 and parameters: {'learning_rate': 0.00015264276529426257, 'lambda_l1': 0.11314716091017404, 'lambda_l2': 2.600924188596275e-06, 'num_leaves': 33, 'feature_fraction': 0.4408019690692587, 'bagging_fraction': 0.7477520196851026, 'bagging_freq': 7, 'min_child_samples': 39}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:00,052] Trial 4 finished with value: 0.1361093184384272 and parameters: {'learning_rate': 0.00367051347848394, 'lambda_l1': 8.074590577113124e-06, 'lambda_l2': 0.001731456655596435, 'num_leaves': 188, 'feature_fraction': 0.9767323299969008, 'bagging_fraction': 0.818224831024344, 'bagging_freq': 4, 'min_child_samples': 21}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:05,427] Trial 5 finished with value: 0.15059548035289919 and parameters: {'learning_rate': 0.009484852900023896, 'lambda_l1': 0.0038115970827088804, 'lambda_l2': 0.0009868845910542812, 'num_leaves': 220, 'feature_fraction': 0.7058646360409433, 'bagging_fraction': 0.5637631632577252, 'bagging_freq': 7, 'min_child_samples': 15}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:09,346] Trial 6 finished with value: 0.15441472914776694 and parameters: {'learning_rate': 0.00027526907313268976, 'lambda_l1': 0.0014854493128853197, 'lambda_l2': 4.183338241676474e-08, 'num_leaves': 107, 'feature_fraction': 0.7000932134050754, 'bagging_fraction': 0.7741127489758577, 'bagging_freq': 7, 'min_child_samples': 32}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:12,677] Trial 7 finished with value: 0.15767805324326795 and parameters: {'learning_rate': 0.09700692741713485, 'lambda_l1': 0.0017169535776221246, 'lambda_l2': 0.0012013476891894251, 'num_leaves': 252, 'feature_fraction': 0.5404330029640093, 'bagging_fraction': 0.5732838832272606, 'bagging_freq': 1, 'min_child_samples': 95}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:16,223] Trial 8 finished with value: 0.1524854629552743 and parameters: {'learning_rate': 0.0031637932291063865, 'lambda_l1': 2.793297828411667e-07, 'lambda_l2': 2.001527329273138e-06, 'num_leaves': 84, 'feature_fraction': 0.7740836065767249, 'bagging_fraction': 0.9635531576313905, 'bagging_freq': 3, 'min_child_samples': 96}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:19,104] Trial 9 finished with value: 0.15586837041058796 and parameters: {'learning_rate': 0.0009186221500084985, 'lambda_l1': 0.0003289693940020606, 'lambda_l2': 0.013059960549992256, 'num_leaves': 142, 'feature_fraction': 0.8737976644087099, 'bagging_fraction': 0.41455296750202364, 'bagging_freq': 1, 'min_child_samples': 63}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:20,551] Trial 10 finished with value: 0.1594722071074727 and parameters: {'learning_rate': 0.01883058348815944, 'lambda_l1': 2.2089673365598266e-08, 'lambda_l2': 8.986934994323596, 'num_leaves': 13, 'feature_fraction': 0.5767358365646056, 'bagging_fraction': 0.5633071489942828, 'bagging_freq': 5, 'min_child_samples': 7}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:21,451] Trial 11 finished with value: 0.14308765501038023 and parameters: {'learning_rate': 0.012950579430973814, 'lambda_l1': 1.2869160101081837e-08, 'lambda_l2': 5.2336528079189515, 'num_leaves': 2, 'feature_fraction': 0.577023273641877, 'bagging_fraction': 0.5664587719901789, 'bagging_freq': 5, 'min_child_samples': 6}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:23,915] Trial 12 finished with value: 0.15985205832393193 and parameters: {'learning_rate': 0.026644128032898784, 'lambda_l1': 1.1438631038191338e-08, 'lambda_l2': 8.180969135648931, 'num_leaves': 56, 'feature_fraction': 0.5831600688969307, 'bagging_fraction': 0.49283652262709127, 'bagging_freq': 5, 'min_child_samples': 66}. Best is trial 0 with value: 0.1601477163166451.\n",
      "[I 2024-11-22 22:42:26,770] Trial 13 finished with value: 0.1620477377136279 and parameters: {'learning_rate': 0.033669703370034346, 'lambda_l1': 7.50203316282404e-06, 'lambda_l2': 0.1719180476748699, 'num_leaves': 62, 'feature_fraction': 0.469971089883368, 'bagging_fraction': 0.48346739365865166, 'bagging_freq': 3, 'min_child_samples': 63}. Best is trial 13 with value: 0.1620477377136279.\n",
      "[I 2024-11-22 22:42:29,932] Trial 14 finished with value: 0.15826910301573727 and parameters: {'learning_rate': 0.04191477108218272, 'lambda_l1': 2.592478378360601e-05, 'lambda_l2': 0.12782361859615707, 'num_leaves': 74, 'feature_fraction': 0.4187165385088959, 'bagging_fraction': 0.6258656425735071, 'bagging_freq': 3, 'min_child_samples': 60}. Best is trial 13 with value: 0.1620477377136279.\n",
      "[I 2024-11-22 22:42:32,867] Trial 15 finished with value: 0.1585120233344348 and parameters: {'learning_rate': 0.0069022396256867655, 'lambda_l1': 4.738070431397893e-06, 'lambda_l2': 0.17330435294335408, 'num_leaves': 161, 'feature_fraction': 0.49853169329666125, 'bagging_fraction': 0.48187245513202326, 'bagging_freq': 2, 'min_child_samples': 76}. Best is trial 13 with value: 0.1620477377136279.\n",
      "[I 2024-11-22 22:42:35,145] Trial 16 finished with value: 0.16134802583585633 and parameters: {'learning_rate': 0.0388508016492705, 'lambda_l1': 8.44318051993065, 'lambda_l2': 0.18972608520704043, 'num_leaves': 58, 'feature_fraction': 0.6235821986306109, 'bagging_fraction': 0.6525967581424221, 'bagging_freq': 3, 'min_child_samples': 78}. Best is trial 13 with value: 0.1620477377136279.\n",
      "[I 2024-11-22 22:42:37,229] Trial 17 finished with value: 0.1565425077494015 and parameters: {'learning_rate': 0.001289459823178381, 'lambda_l1': 7.57853367155638, 'lambda_l2': 0.03266758314606301, 'num_leaves': 51, 'feature_fraction': 0.633383151631402, 'bagging_fraction': 0.6581936200724597, 'bagging_freq': 3, 'min_child_samples': 77}. Best is trial 13 with value: 0.1620477377136279.\n",
      "[I 2024-11-22 22:42:39,638] Trial 18 finished with value: 0.16247064681217246 and parameters: {'learning_rate': 0.05393530722692628, 'lambda_l1': 0.5906720640598918, 'lambda_l2': 0.9079354829661402, 'num_leaves': 35, 'feature_fraction': 0.486237214335383, 'bagging_fraction': 0.8581182837877586, 'bagging_freq': 4, 'min_child_samples': 80}. Best is trial 18 with value: 0.16247064681217246.\n",
      "[I 2024-11-22 22:42:42,308] Trial 19 finished with value: 0.16198555825538802 and parameters: {'learning_rate': 0.06831000691922536, 'lambda_l1': 0.3313516907884902, 'lambda_l2': 0.9695075258965664, 'num_leaves': 36, 'feature_fraction': 0.4889408789914146, 'bagging_fraction': 0.9114190493658683, 'bagging_freq': 4, 'min_child_samples': 86}. Best is trial 18 with value: 0.16247064681217246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 20\n",
      "Best trial:\n",
      "  Value: 0.16247064681217246\n",
      "  Params: \n",
      "    learning_rate: 0.05393530722692628\n",
      "    lambda_l1: 0.5906720640598918\n",
      "    lambda_l2: 0.9079354829661402\n",
      "    num_leaves: 35\n",
      "    feature_fraction: 0.486237214335383\n",
      "    bagging_fraction: 0.8581182837877586\n",
      "    bagging_freq: 4\n",
      "    min_child_samples: 80\n"
     ]
    }
   ],
   "source": [
    "# Define study\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_optimization\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05393530722692628,\n",
       " 'lambda_l1': 0.5906720640598918,\n",
       " 'lambda_l2': 0.9079354829661402,\n",
       " 'num_leaves': 35,\n",
       " 'feature_fraction': 0.486237214335383,\n",
       " 'bagging_fraction': 0.8581182837877586,\n",
       " 'bagging_freq': 4,\n",
       " 'min_child_samples': 80}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a0afb8ffab403e81f4fbbc23e662bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 - Partial AUC Score: 0.15600\n",
      "Fold: 2 - Partial AUC Score: 0.16550\n",
      "Fold: 3 - Partial AUC Score: 0.15009\n",
      "Fold: 4 - Partial AUC Score: 0.16524\n",
      "Fold: 5 - Partial AUC Score: 0.16908\n"
     ]
    }
   ],
   "source": [
    "# Define best model\n",
    "\n",
    "best_params = trial.params\n",
    "best_params['objective'] = 'binary'\n",
    "best_params['metric'] = 'custom'\n",
    "best_params['verbosity'] = -1\n",
    "best_params['boosting_type'] = 'gbdt'\n",
    "best_params['device'] = 'cpu'\n",
    "best_params['random state'] = RANDOM_SEED\n",
    "best_params['n_estimators'] = 1000\n",
    "\n",
    "# Initialize list to store AUC scores\n",
    "aucs = []\n",
    "models = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold in tqdm(range(N_FOLDS), total=N_FOLDS):\n",
    "    # Get train and validation data\n",
    "    train_data = metadata[metadata['kfold'] != fold].reset_index(drop=True)\n",
    "    val_data = metadata[metadata['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # Define features and target\n",
    "    features = train_data[train_cols]\n",
    "    target = train_data['target']\n",
    "\n",
    "    model = VotingClassifier([(f\"lgb_{i}\", lgb.LGBMClassifier(random_state=i, **best_params)) for i in range(3)], voting=\"soft\")\n",
    "    model.fit(features, target)\n",
    "    # Predict probabilities for validation data\n",
    "    preds_proba = model.predict_proba(val_data[train_cols])[:, 1]  # Probability of the positive class\n",
    "    auc = partial_auc_score(val_data['target'], preds_proba)\n",
    "    print(f\"Fold: {fold+1} - Partial AUC Score: {auc:.5f}\")\n",
    "    aucs.append(auc)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Partial AUC for LGBM: 0.16118\n"
     ]
    }
   ],
   "source": [
    "# Mean AUC score\n",
    "mean_auc = np.mean(aucs)\n",
    "print(f\"Mean Partial AUC for LGBM: {mean_auc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save models\n",
    "for i, model in enumerate(models):\n",
    "    dump(model, f\"lgb_model_{i}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for XGBoost\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",  # AUC will still be tracked as an eval metric\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.1, log=True),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 10.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 100),\n",
    "        \"random_state\": RANDOM_SEED\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "\n",
    "    for fold in range(N_FOLDS):\n",
    "        train_data = metadata[metadata['kfold'] != fold].reset_index(drop=True)\n",
    "        val_data = metadata[metadata['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        features = train_data[train_cols]\n",
    "        target = train_data['target']\n",
    "\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "        # Train XGBoost model\n",
    "        dtrain = xgb.DMatrix(features, label=target)\n",
    "        dval = xgb.DMatrix(val_data[train_cols], label=val_data['target'])\n",
    "        evals = [(dval, 'valid')]\n",
    "        \n",
    "        model = xgb.train(param, dtrain, evals=evals, num_boost_round=1000,\n",
    "                          early_stopping_rounds=50, verbose_eval=False)\n",
    "        \n",
    "        preds = model.predict(dval)\n",
    "        auc = partial_auc_score(val_data['target'], preds)\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-22 22:47:22,644] A new study created in memory with name: xgb_optimization\n",
      "[I 2024-11-22 22:47:32,642] Trial 0 finished with value: 0.15971507395240503 and parameters: {'learning_rate': 0.011546540000196066, 'lambda': 2.240993966620505, 'alpha': 1.0929072965331964e-06, 'max_depth': 9, 'subsample': 0.7858816807755473, 'colsample_bytree': 0.5474771462223389, 'min_child_weight': 84}. Best is trial 0 with value: 0.15971507395240503.\n",
      "[I 2024-11-22 22:47:34,661] Trial 1 finished with value: 0.14532371615820266 and parameters: {'learning_rate': 0.0009127325431013793, 'lambda': 2.826464166115527e-07, 'alpha': 0.00015856838250149165, 'max_depth': 4, 'subsample': 0.5757727836828336, 'colsample_bytree': 0.6056020553581936, 'min_child_weight': 99}. Best is trial 0 with value: 0.15971507395240503.\n",
      "[I 2024-11-22 22:47:50,542] Trial 2 finished with value: 0.15935380276061323 and parameters: {'learning_rate': 0.004812012390943991, 'lambda': 1.3158725588979303e-05, 'alpha': 0.0005757184614070826, 'max_depth': 7, 'subsample': 0.948166486443451, 'colsample_bytree': 0.8659421103054941, 'min_child_weight': 66}. Best is trial 0 with value: 0.15971507395240503.\n",
      "[I 2024-11-22 22:47:53,702] Trial 3 finished with value: 0.15395862354399276 and parameters: {'learning_rate': 0.00017215723139366542, 'lambda': 6.173967680482992e-06, 'alpha': 0.00035134172786911266, 'max_depth': 9, 'subsample': 0.8538191136492351, 'colsample_bytree': 0.5101230600735701, 'min_child_weight': 32}. Best is trial 0 with value: 0.15971507395240503.\n",
      "[I 2024-11-22 22:48:00,393] Trial 4 finished with value: 0.16225276417674078 and parameters: {'learning_rate': 0.017573346421714573, 'lambda': 0.00020234202248632944, 'alpha': 0.009761102064932605, 'max_depth': 9, 'subsample': 0.7367569462044208, 'colsample_bytree': 0.445737446186909, 'min_child_weight': 37}. Best is trial 4 with value: 0.16225276417674078.\n",
      "[I 2024-11-22 22:48:04,210] Trial 5 finished with value: 0.1614606372263189 and parameters: {'learning_rate': 0.05917579264620019, 'lambda': 1.6243033369911306e-05, 'alpha': 1.599540386987727e-05, 'max_depth': 5, 'subsample': 0.7142638945457063, 'colsample_bytree': 0.715280020147413, 'min_child_weight': 74}. Best is trial 4 with value: 0.16225276417674078.\n",
      "[I 2024-11-22 22:48:05,777] Trial 6 finished with value: 0.1439244156643355 and parameters: {'learning_rate': 0.0016097646629612008, 'lambda': 3.944335171533476, 'alpha': 3.7385019808201037e-07, 'max_depth': 9, 'subsample': 0.45632268909709817, 'colsample_bytree': 0.9561886492559258, 'min_child_weight': 77}. Best is trial 4 with value: 0.16225276417674078.\n",
      "[I 2024-11-22 22:48:14,436] Trial 7 finished with value: 0.16089704390207865 and parameters: {'learning_rate': 0.01049879739243752, 'lambda': 0.001320696436531879, 'alpha': 1.862615220691889, 'max_depth': 10, 'subsample': 0.5737460301850009, 'colsample_bytree': 0.7903335871194972, 'min_child_weight': 37}. Best is trial 4 with value: 0.16225276417674078.\n",
      "[I 2024-11-22 22:48:17,207] Trial 8 finished with value: 0.1534048783422662 and parameters: {'learning_rate': 0.00026573093769577074, 'lambda': 0.0002592280958085593, 'alpha': 2.0584697843139883e-08, 'max_depth': 7, 'subsample': 0.4967863753364651, 'colsample_bytree': 0.8063110062156658, 'min_child_weight': 15}. Best is trial 4 with value: 0.16225276417674078.\n",
      "[I 2024-11-22 22:48:20,267] Trial 9 finished with value: 0.1603637789079092 and parameters: {'learning_rate': 0.08163288696553753, 'lambda': 0.2215792489202858, 'alpha': 0.0006425295316766081, 'max_depth': 2, 'subsample': 0.5289712009353461, 'colsample_bytree': 0.5057346318100888, 'min_child_weight': 57}. Best is trial 4 with value: 0.16225276417674078.\n",
      "[I 2024-11-22 22:48:26,822] Trial 10 finished with value: 0.16312132150405126 and parameters: {'learning_rate': 0.024387184674775072, 'lambda': 2.7537038054884306e-08, 'alpha': 0.22390734815967694, 'max_depth': 7, 'subsample': 0.6786181909732795, 'colsample_bytree': 0.41387785641011565, 'min_child_weight': 1}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:48:32,284] Trial 11 finished with value: 0.160234102532129 and parameters: {'learning_rate': 0.02383610562303153, 'lambda': 3.085894785463494e-08, 'alpha': 0.27819519374852253, 'max_depth': 7, 'subsample': 0.6671370535083909, 'colsample_bytree': 0.4005686253438388, 'min_child_weight': 2}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:48:37,544] Trial 12 finished with value: 0.16225234008295014 and parameters: {'learning_rate': 0.02415203923762064, 'lambda': 0.006952365565680077, 'alpha': 0.024717824424542744, 'max_depth': 8, 'subsample': 0.6625105929829055, 'colsample_bytree': 0.438595156119582, 'min_child_weight': 31}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:48:41,506] Trial 13 finished with value: 0.1566901630948254 and parameters: {'learning_rate': 0.004678175158138218, 'lambda': 1.06611844380074e-08, 'alpha': 0.03579986449865932, 'max_depth': 5, 'subsample': 0.7775548295881477, 'colsample_bytree': 0.6418498635358431, 'min_child_weight': 5}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:48:47,088] Trial 14 finished with value: 0.1627206884779166 and parameters: {'learning_rate': 0.025394613226921086, 'lambda': 0.02344649517371287, 'alpha': 7.504101143675079, 'max_depth': 10, 'subsample': 0.9532274305630601, 'colsample_bytree': 0.45878064843895994, 'min_child_weight': 46}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:48:50,706] Trial 15 finished with value: 0.15955448013799214 and parameters: {'learning_rate': 0.037972263650762, 'lambda': 0.03548644321694971, 'alpha': 4.1948462869011385, 'max_depth': 3, 'subsample': 0.9665092955812922, 'colsample_bytree': 0.6105482117382003, 'min_child_weight': 20}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:48:54,108] Trial 16 finished with value: 0.1595836425482609 and parameters: {'learning_rate': 0.09618830783865924, 'lambda': 3.4895770587045654e-07, 'alpha': 6.122226824261138, 'max_depth': 10, 'subsample': 0.8941789690642935, 'colsample_bytree': 0.4830694792596931, 'min_child_weight': 51}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:49:01,353] Trial 17 finished with value: 0.16082984763420233 and parameters: {'learning_rate': 0.007042039394326218, 'lambda': 0.16021655243952387, 'alpha': 0.454738622372936, 'max_depth': 6, 'subsample': 0.8354644758591911, 'colsample_bytree': 0.5552180619513296, 'min_child_weight': 17}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:49:08,354] Trial 18 finished with value: 0.15350735166465238 and parameters: {'learning_rate': 0.002019304736573818, 'lambda': 0.002795983292627282, 'alpha': 0.26518176172385166, 'max_depth': 8, 'subsample': 0.9882437108150214, 'colsample_bytree': 0.6935925097822125, 'min_child_weight': 47}. Best is trial 10 with value: 0.16312132150405126.\n",
      "[I 2024-11-22 22:49:09,881] Trial 19 finished with value: 0.1484125132221453 and parameters: {'learning_rate': 0.000645477763966754, 'lambda': 2.3113306926387625e-06, 'alpha': 0.004557016987207461, 'max_depth': 6, 'subsample': 0.6165806300090589, 'colsample_bytree': 0.40200521153147284, 'min_child_weight': 98}. Best is trial 10 with value: 0.16312132150405126.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 20\n",
      "Best trial:\n",
      "  Value: 0.16312132150405126\n",
      "  Params: \n",
      "    learning_rate: 0.024387184674775072\n",
      "    lambda: 2.7537038054884306e-08\n",
      "    alpha: 0.22390734815967694\n",
      "    max_depth: 7\n",
      "    subsample: 0.6786181909732795\n",
      "    colsample_bytree: 0.41387785641011565\n",
      "    min_child_weight: 1\n"
     ]
    }
   ],
   "source": [
    "# Define study\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"xgb_optimization\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45c332e50724c9299cd337db31a01ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 - AUC Score: 0.16119\n",
      "Fold 1 - AUC Score: 0.16821\n",
      "Fold 2 - AUC Score: 0.15332\n",
      "Fold 3 - AUC Score: 0.16266\n",
      "Fold 4 - AUC Score: 0.16597\n",
      "Average AUC Score across folds: 0.16227\n"
     ]
    }
   ],
   "source": [
    "# Define best model\n",
    "best_params = trial.params\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['eval_metric'] = 'auc'\n",
    "best_params['booster'] = 'gbtree'\n",
    "best_params['n_estimators'] = 1000\n",
    "\n",
    "# Initialize list to store AUC scores and models for each fold\n",
    "aucs = []\n",
    "models = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold in tqdm(range(N_FOLDS)):\n",
    "    # Get train and validation data for this fold\n",
    "    train_data = metadata[metadata['kfold'] != fold].reset_index(drop=True)\n",
    "    val_data = metadata[metadata['kfold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # Define features and target\n",
    "    X_train, y_train = train_data[train_cols], train_data['target']\n",
    "    X_val, y_val = val_data[train_cols], val_data['target']\n",
    "\n",
    "    model = VotingClassifier([(f\"xgb_{i}\", xgb.XGBClassifier(random_state=i, **best_params)) for i in range(3)], voting=\"soft\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    # Calculate AUC score\n",
    "    auc = partial_auc_score(y_val, preds)\n",
    "    print(f\"Fold {fold} - AUC Score: {auc:.5f}\")\n",
    "    \n",
    "    # Append results\n",
    "    aucs.append(auc)\n",
    "    models.append(model)\n",
    "\n",
    "# Display average AUC across folds\n",
    "print(f\"Average AUC Score across folds: {np.mean(aucs):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    dump(model, f\"xgb_model_{i}.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
