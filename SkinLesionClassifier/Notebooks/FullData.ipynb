{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import io\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, TargetEncoder\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "RANDOM_SEED = 42\n",
    "N_FOLDS = 5\n",
    "maj_frac = 0.1\n",
    "min_frac = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available :(\n",
      "/CPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"GPU\", \"available (YES!)\" if tf.config.list_physical_devices('GPU') else \"not available :(\")\n",
    "\n",
    "# If GPU is available, use it\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device = '/GPU:0'\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "metadata_df = pd.read_csv('train_processed.csv')\n",
    "train_val_img = h5py.File(f'train-image.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Data Generator\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "class TestDataLoader:\n",
    "    def __init__(self, hdf5_file, metadata_df, batch_size=32):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.metadata_df = metadata_df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of full batches\n",
    "        return (len(self.metadata_df) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "        # If the end index exceeds the data length, adjust it for the last batch\n",
    "        batch_indices = range(start_idx, min(end_idx, len(self.metadata_df)))\n",
    "        \n",
    "        images = []\n",
    "        for index in batch_indices:\n",
    "            isic_id = self.metadata_df.iloc[index]['isic_id']\n",
    "            img = self.load_image(isic_id)\n",
    "            images.append(img)\n",
    "\n",
    "        return np.array(images)\n",
    "\n",
    "    def load_image(self, isic_id):\n",
    "        byte_string = self.hdf5_file[isic_id][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        img = Image.open(io.BytesIO(nparr))\n",
    "        img = img.resize((128, 128))\n",
    "        img = preprocess_input(np.array(img))\n",
    "        return img\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def partial_auc_metric(y_true, y_pred):\n",
    "    def compute_partial_auc(y_true, y_pred):\n",
    "        # Convert tensors to numpy arrays\n",
    "        y_true_np = y_true.numpy()\n",
    "        y_pred_np = y_pred.numpy()\n",
    "\n",
    "        # Compute partial AUC as per your custom function\n",
    "        min_tpr = 0.80\n",
    "        max_fpr = abs(1 - min_tpr)\n",
    "        \n",
    "        v_gt = abs(y_true_np - 1)\n",
    "        v_pred = np.array([1.0 - x for x in y_pred_np])\n",
    "        \n",
    "        partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "        partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "        \n",
    "        return partial_auc\n",
    "\n",
    "    # Wrap the custom Python function with tf.py_function\n",
    "    result = tf.py_function(func=compute_partial_auc, inp=[y_true, y_pred], Tout=tf.float32)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# Load the models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load models with custom metric\n",
    "model_paths = ['Fold1/model_11.h5', 'Fold2/model_03.h5', 'Fold3/model_06.h5', 'Fold4/model_09.h5', 'Fold5/model_02.h5']\n",
    "models = [load_model(path, custom_objects={'partial_auc_metric': partial_auc_metric}) for path in model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each model using SavedModel format\n",
    "for i, model in enumerate(models):\n",
    "    model_json = model.to_json()\n",
    "    with open(f\"model_{i+1}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    model.save_weights(f\"model_{i+1}.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to make predictions to avoid retracing\n",
    "@tf.function\n",
    "def make_predictions(model, batch):\n",
    "    return model(batch, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df1800e7ab64936b73af7b863af5adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/12534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 13:56:50.338216: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1928 calls to <function make_predictions at 0x34d77fd80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Define data loader\n",
    "test_loader = TestDataLoader(train_val_img, metadata_df)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: test_loader,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Estimate the number of batches in the test dataset\n",
    "num_batches = len(test_loader)\n",
    "\n",
    "# Initialize prediction dictionary\n",
    "predictions = {f'M{i+1}': [] for i in range(len(models))}\n",
    "\n",
    "for batch in tqdm(test_dataset, total=num_batches, desc='Predicting'):\n",
    "    for i, model in enumerate(models):\n",
    "        predictions[f'M{i+1}'].append(make_predictions(model, batch))\n",
    "\n",
    "# Convert predictions to numpy arrays\n",
    "for key in predictions.keys():\n",
    "    predictions[key] = np.concatenate([p.numpy() for p in predictions[key]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pAUC: 0.13719458878040314\n"
     ]
    }
   ],
   "source": [
    "# Calculate pAUC for each model\n",
    "paucs = []\n",
    "\n",
    "for key in predictions.keys():\n",
    "    pauc = partial_auc_metric(metadata_df['target'], predictions[key])\n",
    "    paucs.append(pauc)\n",
    "\n",
    "# Calculate the mean pAUC\n",
    "mean_pauc = np.mean(paucs)\n",
    "print(f'Mean pAUC: {mean_pauc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1: (401059, 1)\n",
      "M2: (401059, 1)\n",
      "M3: (401059, 1)\n",
      "M4: (401059, 1)\n",
      "M5: (401059, 1)\n"
     ]
    }
   ],
   "source": [
    "for key in predictions.keys():\n",
    "    print(f'{key}: {predictions[key].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack predictions from all models along a new axis and calculate the mean across this axis\n",
    "# This will result in an array of shape (401059, 1)\n",
    "cnn_confidence = np.mean(\n",
    "    np.stack([predictions['M1'], predictions['M2'], predictions['M3'], predictions['M4'], predictions['M5']], axis=1),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cnn_confidence as a new column in metadata_df\n",
    "metadata_df['cnn_confidence'] = cnn_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    401059.000000\n",
       "mean          0.196883\n",
       "std           0.214336\n",
       "min           0.000030\n",
       "25%           0.031529\n",
       "50%           0.113297\n",
       "75%           0.296966\n",
       "max           0.998634\n",
       "Name: cnn_confidence, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['cnn_confidence'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the CNN confidence\n",
    "noise_level = 0.015\n",
    "\n",
    "metadata_df['cnn_confidence'] = metadata_df['cnn_confidence'] + np.random.normal(0, noise_level, metadata_df.shape[0])\n",
    "metadata_df['cnn_confidence'] = metadata_df['cnn_confidence'].clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add folds using StratifiedGroupKFold\n",
    "metadata_df['fold'] = -1\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_FOLDS)\n",
    "\n",
    "for fold, (train_val_idx, test_idx) in enumerate(sgkf.split(metadata_df, metadata_df['target'], metadata_df['patient_id'])):\n",
    "    metadata_df.loc[test_idx, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encoding for 'sex' column\n",
    "le = LabelEncoder()\n",
    "metadata_df['sex_encoded'] = le.fit_transform(metadata_df['sex'])\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "metadata_df['age_group'] = ordinal_encoder.fit_transform(metadata_df['age_group'].values.reshape(-1, 1))\n",
    "\n",
    "X = metadata_df[['tbp_lv_location_simple']]\n",
    "y = metadata_df['target']\n",
    "\n",
    "# Initialize the TargetEncoder with smoothing (adjust `smooth` and `cv` as needed)\n",
    "encoder = TargetEncoder(smooth='auto', cv=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_encoded = encoder.fit_transform(X, y)\n",
    "\n",
    "# Add the encoded feature back to the DataFrame\n",
    "metadata_df['tbp_location_encoded'] = X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['isic_id', 'patient_id', 'target', 'fold', 'sex', 'tbp_lv_location_simple']\n",
    "train_cols = [col for col in metadata_df.columns if col not in cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pAUC(y_true, y_pred):\n",
    "    # Compute partial AUC as per your custom function\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "    \n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = np.array([1.0 - x for x in y_pred])\n",
    "    \n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return partial_auc\n",
    "\n",
    "# Define the custom metric\n",
    "def custom_lgbm_metric(y_true, y_pred):\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "\n",
    "    # Convert y_true to 0/1 format for binary classification\n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = np.array([1.0 - x for x in y_pred])\n",
    "\n",
    "    # Calculate partial AUC\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "\n",
    "    # Return the metric name, value, and maximization indicator\n",
    "    return 'PAUC', partial_auc, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "def lgbm_objective(trial):\n",
    "    # Define hyperparameters\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"custom\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"device\": \"cpu\",\n",
    "        'random state': RANDOM_SEED\n",
    "    }\n",
    "\n",
    "    # Initialize list to store AUC scores\n",
    "    aucs = []\n",
    "\n",
    "    # Iterate over folds\n",
    "    for fold in range(N_FOLDS):\n",
    "        # Get train and validation data\n",
    "        train_data = metadata_df[metadata_df['fold'] != fold].reset_index(drop=True)\n",
    "        val_data = metadata_df[metadata_df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Define features and target\n",
    "        features = train_data[train_cols]\n",
    "        target = train_data['target']\n",
    "\n",
    "        dtrain = lgb.Dataset(features, target)\n",
    "\n",
    "        # Train model\n",
    "        model = lgb.train(param, dtrain,)\n",
    "        preds = model.predict(val_data[train_cols])\n",
    "        auc = pAUC(val_data['target'], preds)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 16:26:15,444] A new study created in memory with name: lgbm_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 16:26:42,431] Trial 0 finished with value: 0.16240818240489427 and parameters: {'learning_rate': 0.004374159873139222, 'lambda_l1': 0.05311456051183232, 'lambda_l2': 0.0032222805283885917, 'num_leaves': 178, 'feature_fraction': 0.5323838651412293, 'bagging_fraction': 0.8554252074635839, 'bagging_freq': 2, 'min_child_samples': 83}. Best is trial 0 with value: 0.16240818240489427.\n",
      "[I 2024-11-12 16:26:58,789] Trial 1 finished with value: 0.16712554894013065 and parameters: {'learning_rate': 0.013071429501476318, 'lambda_l1': 1.9873439946964387e-06, 'lambda_l2': 0.012795509170231572, 'num_leaves': 252, 'feature_fraction': 0.8074594807870954, 'bagging_fraction': 0.8191371292001413, 'bagging_freq': 4, 'min_child_samples': 100}. Best is trial 1 with value: 0.16712554894013065.\n",
      "[I 2024-11-12 16:27:13,111] Trial 2 finished with value: 0.1584134751994358 and parameters: {'learning_rate': 0.0006108119884128023, 'lambda_l1': 2.4526872253941352e-05, 'lambda_l2': 0.023008593108168527, 'num_leaves': 190, 'feature_fraction': 0.6177818863834068, 'bagging_fraction': 0.6961863349693382, 'bagging_freq': 6, 'min_child_samples': 6}. Best is trial 1 with value: 0.16712554894013065.\n",
      "[I 2024-11-12 16:27:29,334] Trial 3 finished with value: 0.16166746249248518 and parameters: {'learning_rate': 0.0015310629453963547, 'lambda_l1': 0.020214528945893394, 'lambda_l2': 1.0412222415551123e-05, 'num_leaves': 236, 'feature_fraction': 0.8397455762493227, 'bagging_fraction': 0.557226352840096, 'bagging_freq': 3, 'min_child_samples': 16}. Best is trial 1 with value: 0.16712554894013065.\n",
      "[I 2024-11-12 16:27:42,344] Trial 4 finished with value: 0.16930079006458496 and parameters: {'learning_rate': 0.00012479547467504027, 'lambda_l1': 0.012807064859475307, 'lambda_l2': 0.6869073867133828, 'num_leaves': 82, 'feature_fraction': 0.6466877807615072, 'bagging_fraction': 0.8707318861170856, 'bagging_freq': 3, 'min_child_samples': 95}. Best is trial 4 with value: 0.16930079006458496.\n",
      "[I 2024-11-12 16:28:03,972] Trial 5 finished with value: 0.16174632816764048 and parameters: {'learning_rate': 0.0004358311819953053, 'lambda_l1': 0.0031791993978841637, 'lambda_l2': 1.2404237001188686e-05, 'num_leaves': 188, 'feature_fraction': 0.6580595595527475, 'bagging_fraction': 0.6952955584614019, 'bagging_freq': 5, 'min_child_samples': 86}. Best is trial 4 with value: 0.16930079006458496.\n",
      "[I 2024-11-12 16:28:19,423] Trial 6 finished with value: 0.1568691296919896 and parameters: {'learning_rate': 0.007342211240203333, 'lambda_l1': 0.0007877477920655501, 'lambda_l2': 0.00416004282740455, 'num_leaves': 191, 'feature_fraction': 0.4431995592611195, 'bagging_fraction': 0.4273101091417703, 'bagging_freq': 5, 'min_child_samples': 17}. Best is trial 4 with value: 0.16930079006458496.\n",
      "[I 2024-11-12 16:28:29,126] Trial 7 finished with value: 0.163658780101101 and parameters: {'learning_rate': 0.00011213076862820028, 'lambda_l1': 0.003786162563029339, 'lambda_l2': 5.220401240717265, 'num_leaves': 238, 'feature_fraction': 0.43922020039807824, 'bagging_fraction': 0.42141129395644095, 'bagging_freq': 7, 'min_child_samples': 83}. Best is trial 4 with value: 0.16930079006458496.\n",
      "[I 2024-11-12 16:28:49,317] Trial 8 finished with value: 0.10848243041792564 and parameters: {'learning_rate': 0.060006873958412574, 'lambda_l1': 7.973395105484722e-07, 'lambda_l2': 5.350616695857372e-08, 'num_leaves': 196, 'feature_fraction': 0.9960775423993419, 'bagging_fraction': 0.5532503403245786, 'bagging_freq': 5, 'min_child_samples': 41}. Best is trial 4 with value: 0.16930079006458496.\n",
      "[I 2024-11-12 16:28:58,605] Trial 9 finished with value: 0.16767686745105212 and parameters: {'learning_rate': 0.0030897756775608077, 'lambda_l1': 5.96685564240212, 'lambda_l2': 0.30653053465817787, 'num_leaves': 43, 'feature_fraction': 0.7438149810564031, 'bagging_fraction': 0.664447837496529, 'bagging_freq': 1, 'min_child_samples': 84}. Best is trial 4 with value: 0.16930079006458496.\n",
      "[I 2024-11-12 16:29:13,362] Trial 10 finished with value: 0.16537290162389645 and parameters: {'learning_rate': 0.00010735195779398279, 'lambda_l1': 2.8868216518565985e-08, 'lambda_l2': 7.610822519255403, 'num_leaves': 73, 'feature_fraction': 0.92011467562513, 'bagging_fraction': 0.9698412041319167, 'bagging_freq': 3, 'min_child_samples': 57}. Best is trial 4 with value: 0.16930079006458496.\n",
      "[I 2024-11-12 16:29:21,864] Trial 11 finished with value: 0.17495870069767475 and parameters: {'learning_rate': 0.03947661966596745, 'lambda_l1': 6.542171607127298, 'lambda_l2': 0.7407506117943363, 'num_leaves': 20, 'feature_fraction': 0.7611166336769638, 'bagging_fraction': 0.8202373097760443, 'bagging_freq': 1, 'min_child_samples': 63}. Best is trial 11 with value: 0.17495870069767475.\n",
      "[I 2024-11-12 16:29:28,468] Trial 12 finished with value: 0.17329891373590572 and parameters: {'learning_rate': 0.07774515824708493, 'lambda_l1': 7.154928548728885, 'lambda_l2': 0.46571462394964, 'num_leaves': 3, 'feature_fraction': 0.7194138626916143, 'bagging_fraction': 0.8564524692876513, 'bagging_freq': 1, 'min_child_samples': 58}. Best is trial 11 with value: 0.17495870069767475.\n",
      "[I 2024-11-12 16:29:34,844] Trial 13 finished with value: 0.1701639431941676 and parameters: {'learning_rate': 0.09653447365854324, 'lambda_l1': 4.394005416379588, 'lambda_l2': 0.00015999801447652445, 'num_leaves': 2, 'feature_fraction': 0.7572071387637531, 'bagging_fraction': 0.9966401948622668, 'bagging_freq': 1, 'min_child_samples': 61}. Best is trial 11 with value: 0.17495870069767475.\n",
      "[I 2024-11-12 16:29:41,443] Trial 14 finished with value: 0.17159029289762545 and parameters: {'learning_rate': 0.02846691683745411, 'lambda_l1': 0.28326805765036206, 'lambda_l2': 0.1854761446812053, 'num_leaves': 3, 'feature_fraction': 0.8732877590539229, 'bagging_fraction': 0.7895396968754432, 'bagging_freq': 1, 'min_child_samples': 45}. Best is trial 11 with value: 0.17495870069767475.\n",
      "[I 2024-11-12 16:29:56,656] Trial 15 finished with value: 0.17044007647709303 and parameters: {'learning_rate': 0.026352422882307647, 'lambda_l1': 0.45899272014330117, 'lambda_l2': 0.0002512099203083934, 'num_leaves': 114, 'feature_fraction': 0.5799442759178906, 'bagging_fraction': 0.919070422999139, 'bagging_freq': 2, 'min_child_samples': 65}. Best is trial 11 with value: 0.17495870069767475.\n",
      "[I 2024-11-12 16:30:08,366] Trial 16 finished with value: 0.17282115895369873 and parameters: {'learning_rate': 0.04516481357844213, 'lambda_l1': 8.418225883105013, 'lambda_l2': 0.10664672549333722, 'num_leaves': 49, 'feature_fraction': 0.7204946111463938, 'bagging_fraction': 0.7646160272083614, 'bagging_freq': 2, 'min_child_samples': 37}. Best is trial 11 with value: 0.17495870069767475.\n",
      "[I 2024-11-12 16:30:16,762] Trial 17 finished with value: 0.1707524736708414 and parameters: {'learning_rate': 0.016305576865562967, 'lambda_l1': 0.651083103659856, 'lambda_l2': 4.630367850772229e-08, 'num_leaves': 27, 'feature_fraction': 0.5394067313863844, 'bagging_fraction': 0.8940918047069517, 'bagging_freq': 1, 'min_child_samples': 71}. Best is trial 11 with value: 0.17495870069767475.\n",
      "[I 2024-11-12 16:30:31,317] Trial 18 finished with value: 0.17083532451898636 and parameters: {'learning_rate': 0.08680152235954043, 'lambda_l1': 0.00011060868394963495, 'lambda_l2': 2.309488840924864, 'num_leaves': 126, 'feature_fraction': 0.7837368073758573, 'bagging_fraction': 0.6177167571308237, 'bagging_freq': 2, 'min_child_samples': 31}. Best is trial 11 with value: 0.17495870069767475.\n",
      "[I 2024-11-12 16:30:43,491] Trial 19 finished with value: 0.1676733192025477 and parameters: {'learning_rate': 0.00942119320319818, 'lambda_l1': 0.1213584136513864, 'lambda_l2': 0.06549474590827221, 'num_leaves': 90, 'feature_fraction': 0.6920325499008757, 'bagging_fraction': 0.7685091030617439, 'bagging_freq': 4, 'min_child_samples': 72}. Best is trial 11 with value: 0.17495870069767475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 20\n",
      "Best trial:\n",
      "  Value: 0.17495870069767475\n",
      "  Params: \n",
      "    learning_rate: 0.03947661966596745\n",
      "    lambda_l1: 6.542171607127298\n",
      "    lambda_l2: 0.7407506117943363\n",
      "    num_leaves: 20\n",
      "    feature_fraction: 0.7611166336769638\n",
      "    bagging_fraction: 0.8202373097760443\n",
      "    bagging_freq: 1\n",
      "    min_child_samples: 63\n"
     ]
    }
   ],
   "source": [
    "# Define study\n",
    "study = optuna.create_study(direction=\"maximize\", study_name='lgbm_tuning')\n",
    "study.optimize(lgbm_objective, n_trials=20)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "lgbm_trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(lgbm_trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in lgbm_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6538035208e34423bc2e4580a4b69a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 - Partial AUC Score: 0.18219\n",
      "Fold: 2 - Partial AUC Score: 0.17213\n",
      "Fold: 3 - Partial AUC Score: 0.18086\n",
      "Fold: 4 - Partial AUC Score: 0.17114\n",
      "Fold: 5 - Partial AUC Score: 0.17409\n",
      "Mean Partial AUC: 0.17608\n"
     ]
    }
   ],
   "source": [
    "# Use the best hyperparameters\n",
    "best_params = lgbm_trial.params\n",
    "best_params['objective'] = 'binary'\n",
    "best_params['metric'] = 'custom'\n",
    "best_params['verbosity'] = -1\n",
    "best_params['boosting_type'] = 'gbdt'\n",
    "best_params['device'] = 'cpu'\n",
    "best_params['n_estimators'] = 400\n",
    "\n",
    "# Initialize list to store AUC scores\n",
    "aucs = []\n",
    "lgbm_models = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold in tqdm(range(N_FOLDS), total=N_FOLDS):\n",
    "    # Get train and validation data\n",
    "    train_data = metadata_df[metadata_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_data = metadata_df[metadata_df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # Define features and target\n",
    "    features = train_data[train_cols]\n",
    "    target = train_data['target']\n",
    "\n",
    "    model = VotingClassifier([(f\"lgb_{i}\", lgb.LGBMClassifier(random_state=i, **best_params)) for i in range(3)], voting=\"soft\")\n",
    "    model.fit(features, target)\n",
    "    # Predict probabilities for validation data\n",
    "    preds_proba = model.predict_proba(val_data[train_cols])[:, 1]  # Probability of the positive class\n",
    "    auc = pAUC(val_data['target'], preds_proba)\n",
    "    print(f\"Fold: {fold+1} - Partial AUC Score: {auc:.5f}\")\n",
    "    aucs.append(auc)\n",
    "    lgbm_models.append(model)\n",
    "\n",
    "print(f\"Mean Partial AUC: {np.mean(aucs):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for XGBoost\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",  # AUC will still be tracked as an eval metric\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.1, log=True),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 10.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 100),\n",
    "        \"random_state\": RANDOM_SEED\n",
    "    }\n",
    "\n",
    "    aucs = []\n",
    "\n",
    "    for fold in range(N_FOLDS):\n",
    "        train_data = metadata_df[metadata_df['fold'] != fold].reset_index(drop=True)\n",
    "        val_data = metadata_df[metadata_df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        features = train_data[train_cols]\n",
    "        target = train_data['target']\n",
    "\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "        # Train XGBoost model\n",
    "        dtrain = xgb.DMatrix(features, label=target)\n",
    "        dval = xgb.DMatrix(val_data[train_cols], label=val_data['target'])\n",
    "        evals = [(dval, 'valid')]\n",
    "        \n",
    "        model = xgb.train(param, dtrain, evals=evals, num_boost_round=1000,\n",
    "                          early_stopping_rounds=50, verbose_eval=False)\n",
    "        \n",
    "        preds = model.predict(dval)\n",
    "        auc = pAUC(val_data['target'], preds)\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 16:12:02,622] A new study created in memory with name: xgb_tuning\n",
      "[I 2024-11-12 16:12:19,402] Trial 0 finished with value: 0.1607741127845222 and parameters: {'learning_rate': 0.007327441405832055, 'lambda': 0.0014734484040870815, 'alpha': 1.1239934217046211e-07, 'max_depth': 4, 'subsample': 0.9583046087150832, 'colsample_bytree': 0.6469592511927842, 'min_child_weight': 22}. Best is trial 0 with value: 0.1607741127845222.\n",
      "[I 2024-11-12 16:12:34,906] Trial 1 finished with value: 0.1552688550567906 and parameters: {'learning_rate': 0.006047666206177761, 'lambda': 3.79299802523574e-07, 'alpha': 0.0030505962888895412, 'max_depth': 3, 'subsample': 0.6884143670692255, 'colsample_bytree': 0.45439263973453725, 'min_child_weight': 75}. Best is trial 0 with value: 0.1607741127845222.\n",
      "[I 2024-11-12 16:12:50,691] Trial 2 finished with value: 0.16781617708072938 and parameters: {'learning_rate': 0.00031293498531399783, 'lambda': 0.03299412197577287, 'alpha': 0.0009775070499208583, 'max_depth': 5, 'subsample': 0.6485848419749759, 'colsample_bytree': 0.7481400392704977, 'min_child_weight': 42}. Best is trial 2 with value: 0.16781617708072938.\n",
      "[I 2024-11-12 16:13:05,426] Trial 3 finished with value: 0.165715930948214 and parameters: {'learning_rate': 0.08124200395556319, 'lambda': 4.431273736254134e-08, 'alpha': 1.1546683976403876e-07, 'max_depth': 6, 'subsample': 0.49609402088295995, 'colsample_bytree': 0.9772487828888621, 'min_child_weight': 70}. Best is trial 2 with value: 0.16781617708072938.\n",
      "[I 2024-11-12 16:13:20,764] Trial 4 finished with value: 0.05867841504223485 and parameters: {'learning_rate': 0.002015255071152755, 'lambda': 0.2418119166455377, 'alpha': 3.5135188442814913, 'max_depth': 4, 'subsample': 0.7686832571558386, 'colsample_bytree': 0.7601525514499798, 'min_child_weight': 18}. Best is trial 2 with value: 0.16781617708072938.\n",
      "[I 2024-11-12 16:13:36,272] Trial 5 finished with value: 0.1552462252114242 and parameters: {'learning_rate': 0.0013078534401597865, 'lambda': 2.0541111345863366e-07, 'alpha': 0.3433322514595583, 'max_depth': 10, 'subsample': 0.4036335827103984, 'colsample_bytree': 0.6539557509435029, 'min_child_weight': 57}. Best is trial 2 with value: 0.16781617708072938.\n",
      "[I 2024-11-12 16:13:48,843] Trial 6 finished with value: 0.16643156867478712 and parameters: {'learning_rate': 0.06437716600133207, 'lambda': 1.6902544884007894e-08, 'alpha': 0.009425271106326594, 'max_depth': 7, 'subsample': 0.6406331346460563, 'colsample_bytree': 0.9380559581720128, 'min_child_weight': 87}. Best is trial 2 with value: 0.16781617708072938.\n",
      "[I 2024-11-12 16:15:11,896] Trial 7 finished with value: 0.174568508070083 and parameters: {'learning_rate': 0.012874159258057308, 'lambda': 0.044476295648551115, 'alpha': 3.5815691164288624e-06, 'max_depth': 2, 'subsample': 0.8472098084019174, 'colsample_bytree': 0.9954070004585429, 'min_child_weight': 25}. Best is trial 7 with value: 0.174568508070083.\n",
      "[I 2024-11-12 16:15:20,698] Trial 8 finished with value: 0.05178412508163124 and parameters: {'learning_rate': 0.00016463853255374434, 'lambda': 0.7493370018024322, 'alpha': 3.9481857035751404, 'max_depth': 8, 'subsample': 0.8654967816165761, 'colsample_bytree': 0.42330054667900174, 'min_child_weight': 95}. Best is trial 7 with value: 0.174568508070083.\n",
      "[I 2024-11-12 16:15:36,165] Trial 9 finished with value: 0.16336405384559582 and parameters: {'learning_rate': 0.0005394287520720639, 'lambda': 0.0018529533099723827, 'alpha': 2.608874579124376e-07, 'max_depth': 4, 'subsample': 0.9592493593382596, 'colsample_bytree': 0.45780949248584396, 'min_child_weight': 83}. Best is trial 7 with value: 0.174568508070083.\n",
      "[I 2024-11-12 16:16:41,626] Trial 10 finished with value: 0.17473715515482202 and parameters: {'learning_rate': 0.02047560880816246, 'lambda': 8.259665407530091e-06, 'alpha': 1.3177934372072183e-05, 'max_depth': 2, 'subsample': 0.8147958720648942, 'colsample_bytree': 0.8536891145229327, 'min_child_weight': 4}. Best is trial 10 with value: 0.17473715515482202.\n",
      "[I 2024-11-12 16:17:46,164] Trial 11 finished with value: 0.17424290326339076 and parameters: {'learning_rate': 0.019471056111263724, 'lambda': 2.1288779687603008e-05, 'alpha': 1.632045234559358e-05, 'max_depth': 2, 'subsample': 0.8055788458865075, 'colsample_bytree': 0.8497400226967513, 'min_child_weight': 4}. Best is trial 10 with value: 0.17473715515482202.\n",
      "[I 2024-11-12 16:18:59,992] Trial 12 finished with value: 0.17614381617293987 and parameters: {'learning_rate': 0.020258880560938636, 'lambda': 3.290335620382773e-05, 'alpha': 2.1147558243641616e-05, 'max_depth': 2, 'subsample': 0.8602868720210404, 'colsample_bytree': 0.8826795748999703, 'min_child_weight': 2}. Best is trial 12 with value: 0.17614381617293987.\n",
      "[I 2024-11-12 16:19:52,003] Trial 13 finished with value: 0.17576629166268037 and parameters: {'learning_rate': 0.032055792816174024, 'lambda': 7.917811379501505e-06, 'alpha': 2.092478151715333e-05, 'max_depth': 2, 'subsample': 0.8800135993606525, 'colsample_bytree': 0.860148427220027, 'min_child_weight': 1}. Best is trial 12 with value: 0.17614381617293987.\n",
      "[I 2024-11-12 16:20:31,882] Trial 14 finished with value: 0.17431700454570934 and parameters: {'learning_rate': 0.027516339227320694, 'lambda': 2.04458647434978e-05, 'alpha': 0.00010597358247053157, 'max_depth': 3, 'subsample': 0.9946067068217221, 'colsample_bytree': 0.8659920572734695, 'min_child_weight': 34}. Best is trial 12 with value: 0.17614381617293987.\n",
      "[I 2024-11-12 16:20:55,081] Trial 15 finished with value: 0.16871436614365667 and parameters: {'learning_rate': 0.03587438292606569, 'lambda': 0.00013650460777960095, 'alpha': 1.3172133693606564e-08, 'max_depth': 9, 'subsample': 0.9010098014036417, 'colsample_bytree': 0.7953963130861184, 'min_child_weight': 11}. Best is trial 12 with value: 0.17614381617293987.\n",
      "[I 2024-11-12 16:21:09,862] Trial 16 finished with value: 0.1688401552048838 and parameters: {'learning_rate': 0.006295648634272446, 'lambda': 1.444733111967368e-06, 'alpha': 0.00013808677769872602, 'max_depth': 6, 'subsample': 0.7397277809547528, 'colsample_bytree': 0.916681401231485, 'min_child_weight': 55}. Best is trial 12 with value: 0.17614381617293987.\n",
      "[I 2024-11-12 16:21:43,928] Trial 17 finished with value: 0.1737887163050163 and parameters: {'learning_rate': 0.04396961087294973, 'lambda': 8.563072134813327, 'alpha': 0.033283847498585706, 'max_depth': 3, 'subsample': 0.8905116427712038, 'colsample_bytree': 0.563918639653015, 'min_child_weight': 32}. Best is trial 12 with value: 0.17614381617293987.\n",
      "[I 2024-11-12 16:22:13,997] Trial 18 finished with value: 0.17007152143586096 and parameters: {'learning_rate': 0.012105818602348764, 'lambda': 0.0002325011151257758, 'alpha': 1.7042028170468533e-06, 'max_depth': 5, 'subsample': 0.5895513967863647, 'colsample_bytree': 0.6926545367576329, 'min_child_weight': 2}. Best is trial 12 with value: 0.17614381617293987.\n",
      "[I 2024-11-12 16:22:22,763] Trial 19 finished with value: 0.06291557183515403 and parameters: {'learning_rate': 0.002786211293719031, 'lambda': 2.1442336925310803e-06, 'alpha': 5.542379300188007e-05, 'max_depth': 2, 'subsample': 0.9218697295444008, 'colsample_bytree': 0.8056244721449473, 'min_child_weight': 14}. Best is trial 12 with value: 0.17614381617293987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 20\n",
      "Best trial:\n",
      "  Value: 0.17614381617293987\n",
      "  Params: \n",
      "    learning_rate: 0.020258880560938636\n",
      "    lambda: 3.290335620382773e-05\n",
      "    alpha: 2.1147558243641616e-05\n",
      "    max_depth: 2\n",
      "    subsample: 0.8602868720210404\n",
      "    colsample_bytree: 0.8826795748999703\n",
      "    min_child_weight: 2\n"
     ]
    }
   ],
   "source": [
    "# Define study\n",
    "study = optuna.create_study(direction=\"maximize\", study_name='xgb_tuning')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c242789c274be2a249b6d5db4a6719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 - AUC Score: 0.17805\n",
      "Fold 1 - AUC Score: 0.17137\n",
      "Fold 2 - AUC Score: 0.17398\n",
      "Fold 3 - AUC Score: 0.16612\n",
      "Fold 4 - AUC Score: 0.17486\n",
      "Average AUC Score across folds: 0.17287\n"
     ]
    }
   ],
   "source": [
    "# Define best model\n",
    "best_params = trial.params\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['eval_metric'] = 'auc'\n",
    "best_params['booster'] = 'gbtree'\n",
    "best_params['n_estimators'] = 400\n",
    "\n",
    "# Initialize list to store AUC scores and models for each fold\n",
    "aucs = []\n",
    "XGmodels = []\n",
    "\n",
    "# Iterate over folds\n",
    "for fold in tqdm(range(N_FOLDS)):\n",
    "    # Get train and validation data for this fold\n",
    "    train_data = metadata_df[metadata_df['fold'] != fold].reset_index(drop=True)\n",
    "    val_data = metadata_df[metadata_df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # Define features and target\n",
    "    X_train, y_train = train_data[train_cols], train_data['target']\n",
    "    X_val, y_val = val_data[train_cols], val_data['target']\n",
    "\n",
    "    model = VotingClassifier([(f\"xgb_{i}\", xgb.XGBClassifier(random_state=i, **best_params)) for i in range(3)], voting=\"soft\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    # Calculate AUC score\n",
    "    auc = pAUC(y_val, preds)\n",
    "    print(f\"Fold {fold} - AUC Score: {auc:.5f}\")\n",
    "    \n",
    "    # Append results\n",
    "    aucs.append(auc)\n",
    "    XGmodels.append(model)\n",
    "\n",
    "# Display average AUC across folds\n",
    "print(f\"Average AUC Score across folds: {np.mean(aucs):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lgbm_models), len(XGmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
